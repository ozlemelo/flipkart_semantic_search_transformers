{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4189,"sourceType":"datasetVersion","datasetId":2506}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ozlemekici/flipkart-semantic-search-transformers?scriptVersionId=242247349\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Flipkart Product Semantic Search with Transformers ##\nThis notebook demonstrates how to build a semantic search engine for e-commerce products using **Transformer-based** sentence embeddings.\nInstead of keyword matching, we use vector-based similarity to find products that best match the **intent and meaning** of a user query.","metadata":{}},{"cell_type":"markdown","source":"### 1. Import libraries & load data ###","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sentence_transformers import SentenceTransformer, util\nimport numpy as np\n\n# Load your dataset (adjust path if needed)\ndf = pd.read_csv('/kaggle/input/flipkart-products/flipkart_com-ecommerce_sample.csv')\n\n# Quick look at the data\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T22:27:21.498326Z","iopub.execute_input":"2025-05-27T22:27:21.49902Z","iopub.status.idle":"2025-05-27T22:27:22.24967Z","shell.execute_reply.started":"2025-05-27T22:27:21.49899Z","shell.execute_reply":"2025-05-27T22:27:22.248586Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Prepare the model and encode product descriptions ###\nWe use the `all-MiniLM-L6-v2` model from the `sentence-transformers` library to encode product descriptions into semantic vectors.","metadata":{}},{"cell_type":"code","source":"# Load pretrained SentenceTransformer model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Extract product descriptions (handle missing descriptions)\nproduct_descriptions = df['description'].fillna(\"\").tolist()\n\n# Generate embeddings tensor\nembeddings = model.encode(product_descriptions, convert_to_tensor=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T22:29:21.077258Z","iopub.execute_input":"2025-05-27T22:29:21.07773Z","iopub.status.idle":"2025-05-27T22:37:55.996516Z","shell.execute_reply.started":"2025-05-27T22:29:21.0777Z","shell.execute_reply":"2025-05-27T22:37:55.995584Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Define semantic search function ###\nThis function finds and returns the most semantically similar product descriptions to a user query using cosine similarity.","metadata":{}},{"cell_type":"code","source":"def search_products(query, top_k=5):\n    # Encode the user query\n    query_vec = model.encode([query], convert_to_tensor=True)\n    \n    # Compute cosine similarity scores between query and product embeddings\n    cos_scores = util.cos_sim(query_vec, embeddings)[0]\n    \n    # Get top_k highest scores' indices\n    top_results = np.argpartition(-cos_scores.cpu(), range(top_k))[:top_k]\n    \n    print(f\"\\nüîé Search results for: '{query}'\\n\")\n    for idx in top_results:\n        idx = int(idx)  # ensure idx is int\n        print(f\"üìå Product: {df['product_name'].iloc[idx]}\")\n        print(f\"üìù Description: {df['description'].iloc[idx][:150]}...\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T22:38:11.897671Z","iopub.execute_input":"2025-05-27T22:38:11.89806Z","iopub.status.idle":"2025-05-27T22:38:11.904523Z","shell.execute_reply.started":"2025-05-27T22:38:11.898017Z","shell.execute_reply":"2025-05-27T22:38:11.903272Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. Try a Sample Query ###\nYou can change the query to any product-related phrase to see results.","metadata":{}},{"cell_type":"code","source":"search_products(\"wireless bluetooth headphones\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T22:38:15.230312Z","iopub.execute_input":"2025-05-27T22:38:15.230615Z","iopub.status.idle":"2025-05-27T22:38:15.280729Z","shell.execute_reply.started":"2025-05-27T22:38:15.230595Z","shell.execute_reply":"2025-05-27T22:38:15.279877Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Explanations\n\n- We use `np.argpartition` for efficient retrieval of the top-k highest similarity scores.\n- The `.cpu()` call moves the tensor to the CPU before converting it to a NumPy array, since embeddings are PyTorch tensors.\n- We handle missing product descriptions by filling NaNs with empty strings (`.fillna(\"\")`).\n- We explicitly cast `idx` to an integer to avoid indexing errors.\n","metadata":{}},{"cell_type":"markdown","source":"### 5.Distribution of Product Description Lengths\n\nThis plot shows the distribution of the number of characters in product descriptions.  \nIt helps us understand the typical length of descriptions in the dataset and detect any outliers or very short/long descriptions.\n","metadata":{}},{"cell_type":"code","source":"# Remove any missing descriptions before calculating their lengths\ndesc_lengths = df['description'].dropna().str.len()\n\nplt.hist(desc_lengths, bins=30)\nplt.title(\"Distribution of Product Description Lengths\")\nplt.xlabel(\"Number of Characters\")\nplt.ylabel(\"Number of Products\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T22:41:57.657965Z","iopub.execute_input":"2025-05-27T22:41:57.658379Z","iopub.status.idle":"2025-05-27T22:41:57.875676Z","shell.execute_reply.started":"2025-05-27T22:41:57.658355Z","shell.execute_reply":"2025-05-27T22:41:57.874584Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Explanation:**\nWe first drop any rows where the description is missing (NaN), because calculating length on missing values causes warnings. Then we compute the length of each product description and plot the histogram of these lengths to understand their distribution.","metadata":{}},{"cell_type":"markdown","source":"### 8. Next Steps ###\n\n- Wrap the search system into a **web app** using [Gradio](https://gradio.app) or [Streamlit](https://streamlit.io).  \n- **Fine-tune** the transformer model using domain-specific (e.g., e-commerce) data.  \n- Expand the semantic search system to include **images**, **customer reviews**, or **pricing filters**.\n","metadata":{}},{"cell_type":"markdown","source":"# Flipkart Product Semantic Search with Transformers\n\nThis notebook demonstrates a semantic search system for e-commerce products using pre-trained transformer embeddings.\n\n**Dataset:** [Flipkart Products Dataset on Kaggle](https://www.kaggle.com/datasets/PromptCloudHQ/flipkart-products)\n\n**GitHub Repository:** [https://github.com/ozlemelo/flipkart_semantic_search_transformers](https://github.com/ozlemelo/flipkart_semantic_search_transformers)\n\n---\n\nFeel free to check the GitHub repo for more details and usage instructions.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}